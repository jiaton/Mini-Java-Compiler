#! /bin/bash
# UCLA CS 132, Spring 2008.  
set -e
shopt -s nullglob  # Globs can expand to empty lists.

err() {
	for line in "$@" ; do echo "$line" 1>&2 ; done
}

die() {
	err "$@"
	exit 1
}

Trim() {
	tr -d '\t '
}

LineCount() {
	wc -l | Trim
}

HomeworkNumber=1
MainClass=Typecheck
JavaVm=java
JavaCompiler=javac

ParserFileName=minijava
ParserClassName=MiniJava

# A limit on how much code they're allowed to submit.  If their code goes over
# the limit, they're probably submitting extra garbage.
MaxKBytes=65

GeneratedClasses=(${ParserClassName}ParserTokenManager \
	${ParserClassName}ParserConstants ${ParserClassName}Parser \
	Token JavaCharStream TokenMgrError ParseException JTBToolkit)
GeneratedDirs=(visitor syntaxtree)

OkText="Program type checked successfully"
ErrText="Type error"

# ------------------------------------------------------------

HomeworkName="hw${HomeworkNumber}"
TarFileExpectedName="${HomeworkName}.tgz"

OsName=$(uname -s)
ProcessorName=$(uname -p)

if [ "$OsName" == "SunOS" ]; then
	GnuTar=gtar
else
	GnuTar=tar
fi

# ------------------------------------------------------------
# Find out where this script is located.

loc="$0"
while [ -h "$loc" ]; do
	ls=`ls -ld "$loc"`
	link=`expr "$ls" : '.*-> \(.*\)$'`
	if expr "$link" : '/.*' > /dev/null; then
		loc="$link"  # Absolute link
	else
		loc="`dirname "$loc"`/$link"  # Relative link
	fi
done
BaseDir=$(dirname "$loc")

ParserJar="$BaseDir/Support/$ParserFileName-parser.jar"
JavaSecurityPolicyFile="$BaseDir/Support/JavaSecurityPolicy.policy"

OutputDir="$BaseDir/Output"
UnzipDir="$OutputDir/Unzipped"
BytecodeDir="$OutputDir/Bytecode"
TestResultDir="$OutputDir/Tests"

# ------------------------------------------------------------
# Check command-line arguments

[ "$#" -eq 2 ] || die "Usage: $0 <test-case-dir> <submit-tarball>"

TestCaseDir="$1" ; shift
TarFile="$1" ; shift
TarBaseName=$(basename "$TarFile")

[ -f "$TarFile" ] || die \
	"ERROR: The <submit-tarball> path doesn't refer to a valid file: \"$TarFile\"."

[ -d "$TestCaseDir" ] || die \
	"ERROR: The <test-case-dir> path doesn't refer to a valid directory: \"$TestCaseDir\"."

[ "$TarBaseName" == "$TarFileExpectedName" ] || die \
	"ERROR: Your tar file is named \"$TarBaseName\".  It should be" \
	"   named \"$TarFileExpectedName\"."

# ------------------------------------------------------------
# Environment check

# Make sure the directory has everything we expect.

[ -f "$ParserJar" ] || die \
	"ERROR: Couldn't find test script support files." \
	"   Test script base directory = \"$BaseDir\"" \
	"   Couldn't find parser JAR file \"$ParserJar\""

[ -f "$JavaSecurityPolicyFile" ] || die \
	"ERROR: Couldn't find test script support files." \
	"   Test script base directory = \"$BaseDir\"" \
	"   Couldn't find Java security policy file \"$JavaSecurityPolicyFile\""

echo "==============="

# ------------------------------------------------------------
# Clean up any previous output

if [ -d "$OutputDir" ]; then
	echo "Deleting old output directory \"$OutputDir\"..."
	rm -r "$OutputDir"
fi

mkdir "$OutputDir"

# ------------------------------------------------------------
# Unzip the submission tarball.

mkdir "$UnzipDir"

echo "Extracting files from \"$TarFile\"..."
"$GnuTar" zxf "$TarFile" -C "$UnzipDir" || die "ERROR: Unable to extract \"$TarFile\"."

[ -d "$UnzipDir/$HomeworkName" ] || die \
	"ERROR: You submission didn't contain a \"${HomeworkName}\" directory."

# Make sure there's only one directory in there.
for file in "$UnzipDir/"*; do
	f="$(basename "$file")"
	[ "$f" == "$HomeworkName" ] || die \
		"ERROR: Found a file at the top level of the submission tarball:" \
		"   \"$f\"" \
		"All files in the tarball should be in the \"$HomeworkName\" directory."
done

# Make sure there are only Java files or ReadMe.txt
(cd "$UnzipDir" ; find . -type f | \
	sed -e '/\/[A-Za-z0-9_][A-Za-z0-9_]*\.java$/d' -e '/^\.\/'"$HomeworkName"'\/ReadMe\.txt$/d') > "$OutputDir/BadFiles.txt"
NumBadFiles="$(LineCount < "$OutputDir/BadFiles.txt")"
if [ $NumBadFiles -gt 0 ]; then
	echo "ERROR: Found unexpected files in submission:" 2>&1
	head -10 "$OutputDir/BadFiles.txt" | \
	while read BadFile; do
		echo "  \"$BadFile\"" 2>&1
	done
	echo "Expecting only \".java\" files and an optional \"$HomeworkName/ReadMe.txt\"."
	exit 1
fi

# Make sure they didn't include JavaCC output or JTB output.
for c in "${GeneratedClasses[@]}"; do
	[ ! -e "$UnzipDir/$HomeworkName/$c.java" ] || die \
		"ERROR: Found \"$HomeworkName/$c.java\" in submission." \
		"   That file is supposed to be generated by JavaCC or JTB." \
		"   Your submission shouldn't include that file."
done
for d in "${GeneratedDirs[@]}"; do
	[ ! -e "$UnzipDir/$HomeworkName/$d" ] || die \
		"ERROR: Found \"$HomeworkName/$d\" in submission." \
		"   That directory is reserved for JavaCC/JTB generated files." \
		"   Your submission shouldn't include any files in this directory."
done

# Do a sanity check on the number of bytes submitted.
TotalBytes=$(find "$UnzipDir" -name '*.java' | xargs cat | wc -c | Trim)
TotalKBytes=$(( TotalBytes / 1024 ))

# ------------------------------------------------------------
# Compile the program

echo "Compiling program with 'javac'..."

mkdir "$BytecodeDir"

set +e
"$JavaCompiler" "$UnzipDir/$HomeworkName/$MainClass.java" \
	-sourcepath "$UnzipDir/$HomeworkName" \
	-cp "$ParserJar" \
	-d "$BytecodeDir" \
	> "$OutputDir/Compile.txt" 2>&1
CompilerExit=$?
set -e

[ $CompilerExit -eq 0 ] || die "ERROR: Compile failed.  See \"$OutputDir/Compile.txt\"."

# Make sure they don't have any classes that conflict with JavaCC/JTB classes.
for c in "${GeneratedClasses[@]}"; do
	[ ! -e "$BytecodeDir/$c.class" ] || die \
		"ERROR: Found \"$c.class\" in compiled output." \
		"   This conflicts with JavaCC or JTB generated class."
done
for d in "${GeneratedDirs[@]}"; do
	[ ! -e "$BytecodeDir/$d" ] || die \
		"ERROR: Found \"$d\" in compiled output." \
		"   This conflicts with JavaCC or JTB generated package."
done

# ------------------------------------------------------------
# Running tests

# (See calls to 'RunTests' below.)

ResultSummary=()
TotalAchievedPercentage=0
TotalGroupPercentage=0

RunTests() {
	local TestCaseDir="$1" ; shift
	local TestResultDir="$1" ; shift
	local PostExec="$1" ; shift

	local DirPrefixLen="${#TestCaseDir}"
	(( DirPrefixLen += 1 ))  # For the trailing "/"

	mkdir -p "$TestResultDir"

	local ErrorCount=0
	local ValidCount=0
	local ErrorDetectCount=0
	local ValidDetectCount=0
	local BadExitCodeCount=0

	for TestFile in "$TestCaseDir/"*.java; do
		TestFileName=${TestFile:${DirPrefixLen}}

		# If the file has the "// TE" marker in it, then it has a type error.
		if [ -n "$(grep -l "// *TE" "$TestFile")" ]; then
			ExpectError=true
			(( ErrorCount += 1 ))
		else
			ExpectError=false
			(( ValidCount += 1 ))
		fi

		# Take off the ".java" suffix
		TestNameLen=${#TestFileName}
		(( TestNameLen -= 5 ))
		TestName=${TestFileName:0:${TestNameLen}}

		OutputFile="$TestResultDir/$TestName.txt"

		echo -n "$TestName " 
		if $ExpectError; then
			echo -n "[te]"
		else
			echo -n "[ok]"
		fi
		echo -n ": "

		CleanedTestFile="$OutputDir/CleanedTestFile.java"

		# Make a copy of the file without the "//TE" marker.  That way the
		# program can't cheat by just searching for "//TE" in the input.
		sed -e 's&// *TE.*&&g' < "$TestFile" > "$CleanedTestFile"
		if [ -n "$(grep -l "// *TE" "$CleanedTestFile")" ]; then
			die "INTERNAL ERROR: Failed to clean test file \"$TestFile\"."
		fi

		set +e
		"$JavaVm" \
			-Djava.security.manager -Djava.security.policy=="$JavaSecurityPolicyFile" \
			-cp "$BytecodeDir:$ParserJar" \
			< "$CleanedTestFile" > "$OutputFile" 2>&1 \
			"$MainClass"
		ProgramExit=$?
		set -e

		rm "$CleanedTestFile"

		OutputLen=$(LineCount < "$OutputFile")
		if [ $OutputLen != 1 ]; then
			echo "FAIL (output should have exactly one line; found $OutputLen)"
			echo "   See \"$OutputFile\"."
			continue
		fi

		OutputLine=$(cat "$OutputFile")

		if [ "$OutputLine" = "$ErrText" ]; then
			if $ExpectError; then
				(( ErrorDetectCount += 1 ))
				if [ $ProgramExit -ne 0 ]; then
					echo "pass"
				else
					echo "pass? (output ok, but expecting non-zero exit code; found $ProgramExit)"
					echo "  If the input has a type error, the program should exit with"
					echo "  a non-zero exit code (ex: \"System.exit(1);\")."
					(( BadExitCodeCount += 1 ))
				fi
			else
				echo "FAIL"
			fi
		elif [ "$OutputLine" = "$OkText" ]; then
			if $ExpectError; then
				echo "FAIL"
			else
				(( ValidDetectCount += 1 ))
				if [ $ProgramExit -eq 0 ]; then
					echo "pass"
				else
					echo "pass? (output ok, but expecting exit code zero; found $ProgramExit)"
					(( BadExitCodeCount += 1 ))
				fi
			fi
		else
			echo "FAIL (bad output: \"$OutputLine\")"
		fi

	done

	printf -- "==== Results ====\n"
	printf -- "- Valid Cases: %u/%u\n" "$ValidDetectCount" "$ValidCount"
	printf -- "- Error Cases: %u/%u\n" "$ErrorDetectCount" "$ErrorCount"
	if [ $BadExitCodeCount -gt 0 ]; then
		printf -- "  - Bad Exit Code Count: %u\n" "$BadExitCodeCount"
	fi

	eval "$PostExec $ValidCount $ValidDetectCount $ErrorCount $ErrorDetectCount $BadExitCodeCount" 
}

AccumulateGrade() {
	local Group="$1" ; shift
	local GroupPercentage="$1" ; shift
	local ValidCount="$1" ; shift
	local ValidDetectCount="$1" ; shift
	local ErrorCount="$1" ; shift
	local ErrorDetectCount="$1" ; shift
	local BadExitCodeCount="$1" ; shift

	[ "$ValidCount" -gt 0 ] || die "INTERNAL ERROR: Zero 'valid' test cases run."
	[ "$ErrorCount" -gt 0 ] || die "INTERNAL ERROR: Zero 'error' test cases run."

	local ValidDetectRate=$(echo "10k $ValidDetectCount $ValidCount / p" | dc)
	local ErrorDetectRate=$(echo "10k $ErrorDetectCount $ErrorCount / p" | dc)
	local TotalDetectRate=$(echo "10k $ValidDetectRate $ErrorDetectRate * v p" | dc)
	local AchievedPercentage=$(echo "10k $TotalDetectRate $GroupPercentage * p" | dc)

	TotalAchievedPercentage=$(echo "10k $TotalAchievedPercentage $AchievedPercentage + p" | dc)
	TotalGroupPercentage=$(echo "10k $TotalGroupPercentage $GroupPercentage + p" | dc)

	ResultSummary[${#ResultSummary[@]}]=$(printf -- "- %s: [%1.2f%%/%u%%]" "$Group Tests" "$AchievedPercentage" "$GroupPercentage")
	ResultSummary[${#ResultSummary[@]}]=$(printf -- "  - Valid Cases: %u/%u, Ratio: %1.3f" "$ValidDetectCount" "$ValidCount" "$ValidDetectRate")
	ResultSummary[${#ResultSummary[@]}]=$(printf -- "  - Error Cases: %u/%u, Ratio: %1.3f" "$ErrorDetectCount" "$ErrorCount" "$ErrorDetectRate")
	ResultSummary[${#ResultSummary[@]}]=$(printf -- "  - Combined: %1.3f (sqrt(ValidRatio*ErrorRatio))" "$TotalDetectRate")

	if [ $BadExitCodeCount -gt 0 ]; then
		ResultSummary[${#ResultSummary[@]}]=$(printf -- "  - Bad Exit Code Count: " "$BadExitCodeCount")
	fi
}

RunGradedTests() {
	local Group="$1" ; shift
	local GroupPercentage="$1" ; shift
	echo "==== Running Tests: $Group ===="
	RunTests "$TestCaseDir/$Group" "$TestResultDir/$Group" "AccumulateGrade '$Group' $GroupPercentage"
}

if [ -f "$TestCaseDir/GradingMarker.txt" ]; then
	# We're in grader mode.  Send 'AccumulateGrade' as a callback so
	# that we can add up the scores from the public and private test
	# cases.

	[ -d "$TestCaseDir/Public" ] || die "ERROR: Found \"GradingMarker.txt\" (in \"$TestCaseDir\"), but couldn't find the \"Public/\" test case directory."
	[ -d "$TestCaseDir/Private" ] || die "ERROR: Found \"GradingMarker.txt\" (in \"$TestCaseDir\"), but couldn't find the \"Private/\" test case directory."

	RunGradedTests Public 40
	RunGradedTests Private 60

	echo "==== Result Summary ===="
	printf -- "Total: %0.2f%%\n" "$TotalAchievedPercentage"
	for line in "${ResultSummary[@]}"; do
		echo "$line"
	done

else
	# We're NOT in grader mode, so we don't need to compute the student's score.
	# Let 'RunTests' print out a count of failures and leave it at that.

	echo "==== Running Tests ===="
	RunTests "$TestCaseDir" "$TestResultDir" "true"
fi

# File size sanity check.
echo "- Submission Size = $TotalKBytes kB"
if [ $TotalKBytes -gt $MaxKBytes ]; then
	err "" \
		"WARNING: It looks like you submitted too much code.  Make sure you didn't" \
		"include JavaCC/JTB-generated code in your submission." \
		"If you really need to submit all that code, talk to the grader or TA."
fi

# if [ "$OsName" != "SunOS" -o "$ProcessorName" != "sparc" ]; then
# 	err "" \
# 		"WARNING: It looks like you're NOT currently running on a SEASNet server." \
# 		"Make sure you test on a SEASNet server before submitting." \
# 		"   OS = \"$OsName\"  Processor = \"$ProcessorName\""
# fi


