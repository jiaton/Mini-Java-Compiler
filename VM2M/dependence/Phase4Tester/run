#! /bin/bash
# UCLA CS 132, Spring 2008.  
set -e
shopt -s nullglob  # Globs can expand to empty lists.

err() {
	for line in "$@" ; do echo "$line" 1>&2 ; done
}

die() {
	err "$@"
	exit 1
}

Trim() {
	tr -d '\t '
}

LineCount() {
	wc -l | Trim
}

HomeworkNumber=4
MainClass=VM2M
JavaVm=java
JavaCompiler=javac

InputLanguageExtension=".vaporm"

# A limit on how much code they're allowed to submit.  If their code goes over
# the limit, they're probably submitting extra garbage.
MaxKBytes=80

GeneratedClasses=()
GeneratedDirs=(visitor syntaxtree)

# ------------------------------------------------------------

SupportFiles=()

HomeworkName="hw${HomeworkNumber}"
TarFileExpectedName="${HomeworkName}.tgz"

OsName=$(uname -s)
ProcessorName=$(uname -p)

if [ "$OsName" == "SunOS" ]; then
	GnuTar=gtar
	GnuDiff=gdiff
else
	GnuTar=tar
	GnuDiff=diff
fi

# ------------------------------------------------------------
# Find out where this script is located.

loc="$0"
while [ -h "$loc" ]; do
	ls=`ls -ld "$loc"`
	link=`expr "$ls" : '.*-> \(.*\)$'`
	if expr "$link" : '/.*' > /dev/null; then
		loc="$link"  # Absolute link
	else
		loc="`dirname "$loc"`/$link"  # Relative link
	fi
done
BaseDir=$(dirname "$loc")

ParserJar="$BaseDir/Support/vapor-parser.jar"
JavaSecurityPolicyFile="$BaseDir/Support/JavaSecurityPolicy.policy"

SupportFiles=("${SupportFiles[@]}" "$ParserJar" "$JavaSecurityPolicyFile")

OutputDir="$BaseDir/Output"
UnzipDir="$OutputDir/Unzipped"
BytecodeDir="$OutputDir/Bytecode"
TestResultDir="$OutputDir/Tests"

# ------------------------------------------------------------
# Target Language Parser/Interpreter

TargetLanguage=MIPS

MipsInterpreterJar="$BaseDir/Support/mars.jar"

TargetParser() {
	"$JavaVm" -jar "$MipsInterpreterJar" nc a "$@"
}

TargetInterpreter() {
	"$JavaVm" -jar "$MipsInterpreterJar" nc "$@"
}

SupportFiles=("${SupportFiles[@]}" "$MipsInterpreterJar")

# ------------------------------------------------------------
# Check command-line arguments

[ "$#" -eq 2 ] || die "Usage: $0 <test-case-dir> <submit-tarball>"

TestCaseDir="$1" ; shift
TarFile="$1" ; shift
TarBaseName=$(basename "$TarFile")

[ -f "$TarFile" ] || die \
	"ERROR: The <submit-tarball> path doesn't refer to a valid file: \"$TarFile\"."

[ -d "$TestCaseDir" ] || die \
	"ERROR: The <test-case-dir> path doesn't refer to a valid directory: \"$TestCaseDir\"."

[ "$TarBaseName" == "$TarFileExpectedName" ] || die \
	"ERROR: Your tar file is named \"$TarBaseName\".  It should be" \
	"   named \"$TarFileExpectedName\"."

# ------------------------------------------------------------
# Environment check

# Make sure the directory has everything we expect.

for f in "${SupportFiles[@]}"; do
	[ -f "$f" ] || die \
		"ERROR: Couldn't find test script support files." \
		"   Script base directory = \"$BaseDir\"" \
		"   Missing support file  = \"$f\""
done

# ------------------------------------------------------------
# Clean up any previous output

echo "==============="

if [ -d "$OutputDir" ]; then
	echo "Deleting old output directory \"$OutputDir\"..."
	rm -r "$OutputDir"
fi

mkdir "$OutputDir"

# ------------------------------------------------------------
# Unzip the submission tarball.

mkdir "$UnzipDir"

echo "Extracting files from \"$TarFile\"..."
"$GnuTar" zxf "$TarFile" -C "$UnzipDir" || die "ERROR: Unable to extract \"$TarFile\"."

[ -d "$UnzipDir/$HomeworkName" ] || die \
	"ERROR: You submission didn't contain a \"${HomeworkName}\" directory."

# Make sure there's only one directory in there.
for file in "$UnzipDir/"*; do
	f="$(basename "$file")"
	[ "$f" == "$HomeworkName" ] || die \
		"ERROR: Found a file at the top level of the submission tarball:" \
		"   \"$f\"" \
		"All files in the tarball should be in the \"$HomeworkName\" directory."
done

# Make sure there are only Java files or ReadMe.txt
(cd "$UnzipDir" ; find . -type f | \
	sed -e '/\/[A-Za-z0-9_][A-Za-z0-9_]*\.java$/d' -e '/^\.\/'"$HomeworkName"'\/ReadMe\.txt$/d') > "$OutputDir/BadFiles.txt"
NumBadFiles="$(LineCount < "$OutputDir/BadFiles.txt")"
if [ $NumBadFiles -gt 0 ]; then
	echo "ERROR: Found unexpected files in submission:" 2>&1
	head -10 "$OutputDir/BadFiles.txt" | \
	while read BadFile; do
		echo "  \"$BadFile\"" 2>&1
	done
	echo "Expecting only \".java\" files and an optional \"$HomeworkName/ReadMe.txt\"."
	exit 1
fi

# Make sure they didn't include JavaCC output or JTB output.
for c in "${GeneratedClasses[@]}"; do
	[ ! -e "$UnzipDir/$HomeworkName/$c.java" ] || die \
		"ERROR: Found \"$HomeworkName/$c.java\" in submission." \
		"   That file is supposed to be generated by JavaCC or JTB." \
		"   Your submission shouldn't include that file."
done
for d in "${GeneratedDirs[@]}"; do
	[ ! -e "$UnzipDir/$HomeworkName/$d" ] || die \
		"ERROR: Found \"$HomeworkName/$d\" in submission." \
		"   That directory is reserved for JavaCC/JTB generated files." \
		"   Your submission shouldn't include any files in this directory."
done

# Do a sanity check on the number of bytes submitted.
TotalBytes=$(find "$UnzipDir" -name '*.java' | xargs cat | wc -c | Trim)
TotalKBytes=$(( TotalBytes / 1024 ))

# ------------------------------------------------------------
# Compile the program

echo "Compiling program with 'javac'..."

mkdir "$BytecodeDir"

set +e
"$JavaCompiler" "$UnzipDir/$HomeworkName/$MainClass.java" \
	-sourcepath "$UnzipDir/$HomeworkName" \
	-cp "$ParserJar" \
	-d "$BytecodeDir" \
	> "$OutputDir/Compile.txt" 2>&1
CompilerExit=$?
set -e

[ $CompilerExit -eq 0 ] || die "ERROR: Compile failed.  See \"$OutputDir/Compile.txt\"."

# Make sure they don't have any classes that conflict with JavaCC/JTB classes.
for c in "${GeneratedClasses[@]}"; do
	[ ! -e "$BytecodeDir/$c.class" ] || die \
		"ERROR: Found \"$c.class\" in compiled output." \
		"   This conflicts with JavaCC or JTB generated class."
done
for d in "${GeneratedDirs[@]}"; do
	[ ! -e "$BytecodeDir/$d" ] || die \
		"ERROR: Found \"$d\" in compiled output." \
		"   This conflicts with JavaCC or JTB generated package."
done

# ------------------------------------------------------------
# Running tests

# (See calls to 'RunTests' below.)

ResultSummary=()
TotalAchievedPercentage=0
TotalGroupPercentage=0

RunTests() {
	local TestCaseDir="$1" ; shift
	local TestResultDir="$1" ; shift
	local PostExec="$1" ; shift

	local DirPrefixLen="${#TestCaseDir}"
	(( DirPrefixLen += 1 ))  # For the trailing "/"

	mkdir -p "$TestResultDir"

	local TestCount=0
	local PassCount=0

	local TestFile
	for TestFile in "$TestCaseDir/"*"$InputLanguageExtension"; do
		local TestFileName=${TestFile:${DirPrefixLen}}

		# Take off the suffix
		local TestNameLen=${#TestFileName}
		(( TestNameLen -= ${#InputLanguageExtension} ))
		local TestName=${TestFileName:0:${TestNameLen}}

		local ResultBase="$TestResultDir/$TestName"

		local ExpectedOutput="$TestCaseDir/$TestName.out"
		local CompilerStderr="$ResultBase.compiler.err"
		local TargetProgram="$ResultBase.compiler.out"
		local TargetParseError="$ResultBase.parse.err"
		local TargetOutput="$ResultBase.out"
		local OutputDiff="$ResultBase.diff"

		echo -n "$TestName: " 

		# Run their compiler
		set +e
		"$JavaVm" \
			-Djava.security.manager -Djava.security.policy=="$JavaSecurityPolicyFile" \
			-cp "$BytecodeDir:$ParserJar" \
			< "$TestFile" > "$TargetProgram" 2> "$CompilerStderr" \
			"$MainClass"
		local ProgramExit=$?
		set -e

		(( TestCount += 1 ))

		if [ $ProgramExit -ne 0 ]; then
			echo "FAIL (your compiler exited with exit code $ProgramExit)"
			echo "  Output: \"$TargetProgram\""
			echo "  StdErr: \"$CompilerStderr\""
			continue
		fi

		if ! diff "$CompilerStderr" "/dev/null" > /dev/null; then
			echo "FAIL (your compiler wrote something to stderr)"
			echo "  See: \"$CompilerStderr\""
			continue;
		fi

		# Run the target language's parser on their code.
		set +e
		TargetParser "$TargetProgram" > /dev/null 2> "$TargetParseError"
		set -e

		if [ $(wc -c < "$TargetParseError") -ne 0 ]; then
			echo "FAIL (your compiler's output isn't syntactically valid $TargetLanguage)"
			echo "  Your Output: \"$TargetProgram\""
			echo "  Parse Error: \"$TargetParseError\""
			continue
		fi

		# Run the target language's interpreter on their code.
		set +e
		TargetInterpreter "$TargetProgram" > "$TargetOutput" 2>&1
		set -e

		set +e
		"$GnuDiff" -u -- "$ExpectedOutput" "$TargetOutput" > "$OutputDiff"
		local DiffExit=$?
		set -e

		if [ $DiffExit -eq 0 ]; then
			echo "pass"
			(( PassCount += 1 ))
		else
			echo "FAIL"
			echo "  Correct: \"$ExpectedOutput\""
			echo "  Yours:   \"$TargetOutput\""
			echo "  Diff:    \"$OutputDiff\""
		fi

	done

	printf -- "==== Results ====\n"
	printf -- "Passed %u/%u test cases\n" "$PassCount" "$TestCount"

	eval "$PostExec $PassCount $TestCount"
}

AccumulateGrade() {
	local Group="$1" ; shift
	local GroupPercentage="$1" ; shift
	local PassCount="$1" ; shift
	local TestCount="$1" ; shift

	[ "$TestCount" -gt 0 ] || die "INTERNAL ERROR: Zero test cases run."

	local AchievedPercentage=$(echo "10k $PassCount $TestCount / $GroupPercentage * p" | dc)

	TotalAchievedPercentage=$(echo "10k $TotalAchievedPercentage $AchievedPercentage + p" | dc)
	TotalGroupPercentage=$(echo "10k $TotalGroupPercentage $GroupPercentage + p" | dc)

	ResultSummary[${#ResultSummary[@]}]=$(printf -- "- %s: [%1.2f%%/%u%%] (Passed %u/%u tests)" "$Group Tests" "$AchievedPercentage" "$GroupPercentage" "$PassCount" "$TestCount")
}

RunGradedTests() {
	local Group="$1" ; shift
	local GroupPercentage="$1" ; shift
	echo "==== Running Tests: $Group ===="
	RunTests "$TestCaseDir/$Group" "$TestResultDir/$Group" "AccumulateGrade '$Group' $GroupPercentage"
}

if [ -f "$TestCaseDir/GradingMarker.txt" ]; then
	# We're in grader mode.  Send 'AccumulateGrade' as a callback so
	# that we can add up the scores from the public and private test
	# cases.

	[ -d "$TestCaseDir/Public" ] || die "ERROR: Found \"GradingMarker.txt\" (in \"$TestCaseDir\"), but couldn't find the \"Public/\" test case directory."
	[ -d "$TestCaseDir/Private" ] || die "ERROR: Found \"GradingMarker.txt\" (in \"$TestCaseDir\"), but couldn't find the \"Private/\" test case directory."

	RunGradedTests Public 40
	RunGradedTests Private 60

	echo "==== Result Summary ===="
	printf -- "Total: %0.2f%%\n" "$TotalAchievedPercentage"
	for line in "${ResultSummary[@]}"; do
		echo "$line"
	done

else
	# We're NOT in grader mode, so we don't need to compute the student's score.
	# Let 'RunTests' print out a count of failures and leave it at that.

	echo "==== Running Tests ===="
	RunTests "$TestCaseDir" "$TestResultDir" "true"
fi

# File size sanity check.
echo "- Submission Size = $TotalKBytes kB"
if [ $TotalKBytes -gt $MaxKBytes ]; then
	err "" \
		"WARNING: It looks like you submitted too much code.  Make sure you didn't" \
		"include JavaCC/JTB-generated code in your submission." \
		"If you really need to submit all that code, talk to the grader or TA."
fi

# if [ "$OsName" != "SunOS" -o "$ProcessorName" != "sparc" ]; then
# 	err "" \
# 		"WARNING: It looks like you're NOT currently running on a SEASNet server." \
# 		"Make sure you test on a SEASNet server before submitting." \
# 		"   OS = \"$OsName\"  Processor = \"$ProcessorName\""
# fi
